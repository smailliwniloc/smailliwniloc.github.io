\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{calc}
\newtheorem{question}{Question}
\newtheorem*{question*}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{answer}{Answer}
\newtheorem*{answer*}{Answer}

\theoremstyle{definition}
\newtheorem{verify}{Verification}
\newtheorem*{verify*}{Verification}

\numberwithin{equation}{section}


\title{Analysis HW 9}
\author{Colin Williams}

\begin{document}
\maketitle


\section*{Question 1}
Let $X \subset \mathbb{R}^n$ be open. Let $x_0 \in X$ and let $f: X \to \mathbb{R}^m$ be differentiable at $x_0$. Let $Y \subset \mathbb{R}^m$ be open and such that $f(x_0) \in Y$. Finally, let $g: Y \to \mathbb{R}^k$ be differentiable at $f(x_0)$. Prove that $D_j(g \circ f)(x_0) = \sum_{i = 1}^m (D_i g)(f(x_0))(D_j f^i)(x_0)$, where $f^i(x)$ is the $i$-th component of $f(x)$. 

\begin{proof}$ $
\\Recall from class, we have shown that since $f$ is differentiable at $x_0$ and $g$ is differentiable at $f(x_0)$, then $g \circ f$ is differentiable at $x_0$ with derivative $(g \circ f)'(x_0) = g'(f(x_0))f'(x_0)$. Using this with the definition of the partial derivative, we get:
\begin{align*}
D_j(g \circ f)(x_0) = (g \circ f)'(x_0) e_j = g'(f(x_0))f'(x_0) e_j = g'(f(x_0)) D_jf(x_0)
\end{align*}
Then, denoting $f(x) = (f^1(x), f^2(x), \cdots, f^m(x))$, we see that 
\begin{align*}
f(x) &= \sum_{i = 1}^m f^i(x) e_i\\
\implies D_jf(x_0) &= D_j \left( \sum_{i = 1}^m f^i(x_0) e_i \right) &\text{\color{red} This may be bad (unusual) notation?}\\
&= \sum_{i = 1}^m D_j f^i(x_0) e_i\\
&= \bigg( D_j f^1(x_0), D_j f^2(x_0), \cdots, D_j f^m(x_0) \bigg)
\end{align*}
Similarly, since $g$ is differentiable at $f(x_0)$, we can decompose $g$ into the linear combination of its partial derivatives. Let $v \in Y \subset \mathbb{R}^m$, then if $v = (v^1, v^2, \ldots, v^m)$, we get:
\begin{align*}
g'(y)v &= \sum_{i = 1}^m v^i D_i g(y)\\
\implies g'(f(x_0))v &= \sum_{i = 1}^m v^i (D_i g)(f(x_0))\\
\implies g'(f(x_0)) D_jf(x_0) &= \sum_{i = 1}^m D_j f^i(x_0) (D_i g)(f(x_0))\\
&= \sum_{i = 1}^m (D_i g)(f(x_0)) (D_j f^i (x_0))
\end{align*}
Therefore, we have the desired equality. 
\end{proof}

\section*{Question 2}
Let $X \subset \mathbb{R}^n$ be open, and let $f: X \to \mathbb{R}^n$ be continuously differentiable and such that $f'(x)$ is invertible for every $x \in X$. Prove that $f(X)$ is open in $\mathbb{R}^n$.

\begin{proof}$ $
\\The hypotheses of the Inverse Function Theorem are clearly satisfied, so we can use that result to say that for every $x \in X$, there exists some open neighborhood of $x$, say $U_x \subset X$, such that $V_x = f(U_x) \subset f(X)$ is also open. Let us construct these $V_x$'s for each $x \in X$. By definition, we know that every $y_0 \in f(X)$ can be expressed as $y_0 = f(x_0)$ for some $x_0 \in X$. In particular this means that $y_0 \in V_{x_0}$. Therefore, we have 
\begin{align*}
f(X) = \bigcup_{x \in X} V_x
\end{align*}
Where the inclusion $\subset$ follows from the discussion above and the inclusion $\supset$ follows since each $V_x$ is a subset of $f(X)$, so their union must also be a subset. Lastly since each $V_x$ is open and the arbitrary union of open sets is open, then we can conclude that $f(X)$ is also open. 
\end{proof}

\section*{Question 3}
Let $f: \mathbb{R} \to \mathbb{R}$ be given by $f(0) = 0$ and $f(x) = x + 2x^2 \sin(1/x)$ if $x \neq 0$. Prove that $f$ is differentiable and $f'(0) = 1$, but $f$ is NOT injective in any neighborhood of $x = 0$. Why does the Inverse Function Theorem not apply here?


\begin{proof}$ $
\\It is easy to see that $f$ is differentiable at all nonzero inputs since the functions $\{x, x^2, \sin(x), 1/x\}$ are all differentiable for $x \neq 0$, so their sum/product/composition is also differentiable for $x \neq 0$. Therefore, I will use the definition of the derivative to determine the differentiability at the point $x = 0$:
\begin{align*}
\lim_{x \to 0} \frac{f(x) - f(0)}{x - 0} &= \lim_{x \to 0} \frac{x + 2x^2 \sin(1/x)}{x}\\
&= \lim_{x \to 0} 1 + 2x \sin(1/x)
\end{align*}
Since $\sin(\cdot)$ is bounded and $2x \to 0$ as $x \to 0$, then we see that $2x\sin(1/x) \to 0$ as $x \to 0$. Thus, the limit above exists and in fact $f'(0) = 1$. 
\\
\\To see the lack of injectivity, let us first look at the derivative of the function for non-zero $x$-values:
\begin{align*}
f'(x) = 1 + 4x\sin(1/x) - 2\cos(1/x)
\end{align*}
Consider the point $x_k = \frac{1}{2k\pi}$ for $k \in \mathbb{Z} \backslash \{0\}$ which gives
\begin{align*}
f'(x_k) = 1 + \frac{4}{2k\pi}\sin(2k\pi) - 2\cos(2k\pi) = 1 - 2 = -1
\end{align*}
Next, consider the point $x_n = \frac{1}{(2n + 1)\pi}$ for $n \in \mathbb{Z}$ which gives
\begin{align*}
f'(x_n) = 1 + \frac{4}{(2n + 1)\pi}\sin((2n + 1)\pi) - 2\cos((2n + 1)\pi) = 1 + 2 = 3
\end{align*}
Therefore, given some neighborhood of the point $0$, say $U_r(0)$, we can choose $k$ and $n$ large enough [say $|k| > \frac{1}{2\pi r}$ and $|n| > \frac{1}{2\pi r} - \frac{1}{2}$] such that $x_k, x_n \in U_r(0)$. In particular, since $U_r(0)$ is connected, then the open interval connecting $x_k$ and $x_n$ is contained inside of $U_r(0)$. In particular, since $f'(x_k) < 0$ and $f'(x_n) > 0$, using the Intermediate Value Theorem on $f'$ over the interval connecting $x_k$ and $x_n$, we can conclude that there exists some point $x_0 \in U_r(0)$ such that $f'(x_0) = 0$. Furthermore, this point $x_0$ must be a local maximum or minimum (not an inflection point) since the derivative switches signs on either side of that point.
\\
\\WLOG, assume that $x_0$ is a point of maximum (proof is nearly identical with a minimum), then there is an $\varepsilon$-neighborhood of $x_0$ such that $x_1 := x_0 - \varepsilon/2$ and $x_2 := x_0 + \varepsilon/2$ satisfy $f(x_1) < f(x_0)$ and $f(x_2) < f(x_0)$. If $f(x_1) = f(x_2)$ we are done and $f$ is not injective. If not, assume that $f(x_1) < f(x_2) < f(x_0)$. Then, by intermediate value theorem, there exists some $\xi \in (x_1, x_0)$ such that $f(\xi) = f(x_2)$. If $f(x_2) < f(x_1) < f(x_0)$, then by IVT, there exists some $\zeta \in (x_2, x_0)$ such that $f(\zeta) = f(x_1)$. In either case, $f$ is not injective. 
\end{proof}

\begin{answer*}$ $
\\The reason that the Inverse Function Theorem does not apply is because our function $f$ needs to be continuously differentiable. It is, in fact, differentiable, but the derivative is not continuous. Namely, the $\cos(1/x)$ term in the derivative (for $x \neq 0$) does not have a limit as $x \to 0$ (although the derivative at zero is defined and equal to 1). Thus, the function is not continuously differentiable, so we cannot apply the Inverse Function Theorem. 
\end{answer*}


\section*{Question 4}
Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be given by $f(x, y) = (e^x \cos(y), e^x \sin(y))$. Prove that $f$ is continuously differentiable and that $f'(x, y)$ is invertible at every point $(x, y) \in \mathbb{R}^2$. Is $f$ injective? Surjective?

\begin{proof}$ $
\\Let us first calculate the partial derivatives of $f$: 
\begin{align*}
D_1 f(x, y) &= \lim_{t \to 0} \frac{f(x + t, y) - f(x, y)}{t}\\
&= \lim_{t \to 0} \left( \frac{e^{x + t} \cos(y) - e^x \cos(y)}{t}, \frac{e^{x + t} \sin(y) - e^x \sin(y)}{t}\right)\\
&= \lim_{t \to 0} \frac{e^t - 1}{t} (e^x \cos(y), e^x \sin(y))\\
&= \frac{d}{dx}[e^x]_{x = 0} (e^x \cos(y), e^x \sin(y))\\
&= (e^x \cos(y), e^x \sin(y))\\
D_2 f(x, y) &= \lim_{t \to 0} \frac{f(x, y + t) - f(x, y)}{t}\\
&= \lim_{t \to 0} \left( \frac{e^x \cos(y + t) - e^x \cos(y)}{t}, \frac{e^x \sin(y + t) - e^x \sin(y)}{t}\right)\\
&= e^x \left( \lim_{t \to 0} \frac{\cos(y + t) - \cos(y)}{t}, \lim_{t \to 0} \frac{\sin(y + t) - \sin(y)}{t}\right)\\
&= e^x \left( \frac{d}{dy}[\cos(y)], \frac{d}{dy}[\sin(y)] \right)\\
&= (-e^x \sin(y), e^x \cos(y))
\end{align*}
Notice each component function is the product of continuous functions, so in fact each partial derivative of $f$ is continuous. Therefore, since continuous partial derivatives imply continuous differentiability, then we have that $f$ is continuously differentiable. Furthermore, if we have a vector $v = (\alpha, \beta) \in \mathbb{R}^2$, then we see that 
\begin{align*}
f'(x, y)v = \sum_{i = 1}^2 D_i f(x, y) v^i &= \alpha (e^x \cos(y), e^x \sin(y)) + \beta (-e^x \sin(y), e^x \cos(y))\\
&= e^x \bigg(\alpha\cos(y) - \beta \sin(y), \alpha \sin(y) + \beta\cos(y) \bigg)
\end{align*}
If I wish to have $v$ in the kernel of $f'(x, y)$, then this previous quantity must be the zero vector. Since $e^x \neq 0$ for all $x \in \mathbb{R}$, this gives us
\begin{align*}
\alpha\cos(y) - \beta \sin(y) &= 0 &&& \alpha \sin(y) + \beta\cos(y) &= 0\\ 
\end{align*}
If we take $\alpha = 0$, then we are left with $\beta\sin(y) = 0$ and $\beta \cos(y) = 0$. However, $\sin(\cdot)$ and $\cos(\cdot)$ are never zero at the same input. Therefore, for both of these to hold, we must have that $\beta = 0$ as well. Additionally, if we first assume that $\beta = 0$, then we can similarly conclude that $\alpha = 0$. Therefore, assume that $\alpha, \beta \neq 0$ which gives:
\begin{align*}
\frac{\alpha}{\beta} &= \frac{\sin(y)}{\cos(y)} &&& \frac{\alpha}{\beta} &= -\frac{\cos(y)}{\sin(y)}\\
& & \implies \frac{\sin(y)}{\cos(y)} &= \frac{-\cos(y)}{\sin(y)}\\
& & \implies \sin^2(y) &= -\cos^2(y)\\
& & \implies \sin^2(y) + \cos^2(y) &= 0
\end{align*}
However, this last line is a contradiction to the Pythagorean identity for $\sin(\cdot)$ and $\cos(\cdot)$, so we have a contradiction. Therefore, it is not possible for $\alpha$ or $\beta$ to be non-zero (in particular, the issue arose from assuming $\beta$ was non-zero which means we must have $\beta = 0$, but we already showed that this means $\alpha = 0$). Thus, if $v \in \ker(f'(x, y))$, then we must have that $v = (0, 0)$. In other words, $\ker(f'(x, y)) = \{(0,0)\}$ which means that $f'(x, y)$ is invertible for all $(x, y) \in \mathbb{R}^2$. 
\end{proof}

\begin{answer*}$ $
\\First, note that $f$ is not injective since 
\begin{align*}
f(0, 0) = f(0, 2\pi) = (1, 0)
\end{align*}
but $(0, 0) \neq (0, 2\pi)$. Additionally, $f$ is not surjective since there is no pair $(x, y)$ such that $f(x, y) = (0, 0)$. If there were, then 
\begin{align*}
(e^x \cos(y), e^x \sin(y)) &= (0, 0)\\
\implies e^x (\cos(y), \sin(y)) &= (0, 0)\\
\implies (\cos(y), \sin(y)) &= (0, 0)
\end{align*}
However, if we take the Euclidean norm of both sides of this last ``equality" we see that the norm of the left is one and the norm of the right is zero. Therefore, we cannot find such a $y$ that makes this equality hold true, so no pair of points maps to $(0, 0)$ which means that $f$ is not surjective. 
\\
\\If I'm not mistaken, this is an example of the Inverse Function Theorem holding only in a local sense and not a global sense. If we restrict our subset, $U$, of our domain from the Inverse Function Theorem to be any open set where the $y$-coordinates are bounded to an interval of length less than $2\pi$ then there are indeed no inconsistencies with the Inverse Function Theorem. 
\end{answer*}



\end{document}