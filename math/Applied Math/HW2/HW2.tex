\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin=0.5in]{geometry}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{calc}
\newtheorem{question}{Question}
\newtheorem*{question*}{Question}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem{answer}{Answer}
\newtheorem*{answer*}{Answer}

\theoremstyle{definition}
\newtheorem{verify}{Verification}
\newtheorem*{verify*}{Verification}

\numberwithin{equation}{section}


\title{Applied Math HW 2}
\author{Colin Williams}


\begin{document}
\maketitle


$ $
\begin{large}
\\Prove the following properties of matrix norms for $A$ an $m \times n$ matrix for properties 1-4 and an $n \times n$ matrix for properties 5-8. 
\end{large}

\section*{Property 1}
\begin{align*}
||A||_1 := \max_{x \neq 0} \frac{||Ax||_1}{||x||_1} = \max_j \sum_{i = 1}^m |a_{ij}| = ||A^T||_\infty
\end{align*}

\begin{proof}$ $
\\Recall that in class, we have proven that 
\begin{align*}
||A||_\infty = \max_i \sum_{j = 1}^n |a_{ij}|
\end{align*}
Thus, $||A||_\infty$ is the maximum sum over the absolute values of elements of each row of $A$. Therefore, since $A^T$ has rows equal to the columns of $A$, then $||A^T||_\infty$ should be equal to the maximum sum over the absolute values of elements of each column of $A$. In other words,
\begin{align*}
||A^T||_\infty = \max_j \sum_{i = 1}^m |a_{ij}|
\end{align*}
which proves the last equality above. Next, moving to the left side of the desired equality, we get
\begin{align*}
||A||_1 = \max_{x \neq 0} \frac{||Ax||_1}{||x||_1} &= \max_{x \neq 0} \frac{\displaystyle \left|\left|\sum_{i = 1}^n a_i x_i\right|\right|_1}{||x||_1}\\
&\leq \max_{x \neq 0} \frac{\displaystyle \sum_{i = 1}^n ||a_i||_1 |x_i|}{||x||_1} &\text{by Triangle Inequality}\\
&\leq \max_{x \neq 0} \frac{\displaystyle \max_j ||a_j||_1 \sum_{i = 1}^n |x_i|}{||x||_1}\\
&= \max_{x \neq 0} \frac{\displaystyle \max_j ||a_j||_1 ||x||_1}{||x||_1}\\
&= \max_{j} ||a_j||_1\\
\end{align*}
where $(a_j)$ denote the $j$-th column of $A$. Thus, using the definition of the 1-norm, we get
\begin{align*}
||A||_1 &\leq \max_j \sum_{i = 1}^m |a_{ij}|
\end{align*}
Next, let $k$ be the index in the above expression such that $\sum_{i = 1}^m |a_{ij}|$ is maximized when $j = k$ and let $e_k$ denote the $k$-th standard unit vector. If that is the case, then 
\begin{align*}
\max_j \sum_{i = 1}^m |a_{ij}| &= \max_j ||a_j||_1\\
&= ||Ae_k||_1\\
&= \frac{||Ae_k||_1}{||e_k||_1}\\
&\leq \max_{x \neq 0} \frac{||Ax||_1}{||x||_1}\\
&= ||A||_1
\end{align*}
Thus, I have shown that both inequalities hold, so in fact
\begin{align*}
||A||_1 &= \max_j \sum_{i = 1}^m |a_{ij}|
\end{align*}
\end{proof}

\section*{Property 2}
\begin{align*}
||A||_2 := \max_{x \neq 0} \frac{||Ax||_2}{||x||_2} = \sqrt{\lambda_{max}(A^T A)} &&\text{where $\lambda_{max}(B)$ denotes the largest eigenvalue of $B$}
\end{align*}

\begin{proof}$ $
\\
\end{proof}

\section*{Property 3}
\begin{align*}
||A||_2 = ||A^T||_2
\end{align*}

\begin{proof}$ $
\\Taking Property 2 as given, we see that 
\begin{align*}
||A||_2 = \sqrt{\lambda_{max}(A^T A)} && \text{and} && ||A^T||_2 = \sqrt{\lambda_{max}(A A^T)}
\end{align*}
Furthermore, as we have shown in class, the matrix $A^TA$ has real and non-negative eigenvalues. Also, we have shown that $AB$ and $BA$ have the same non-zero eigenvalues for any matrices $A$ and $B$. Therefore, $A^TA$ and $AA^T$ have the same non-zero eigenvalues. Thus, if $\lambda_{max}(A^TA) > 0$, then we know that $\lambda_{max}(A^TA) = \lambda_{max}(AA^T)$. If $\lambda_{max}(A^TA) = 0$. Then we know that all eigenvalues of $A^TA$ are zero. This also means that $AA^T$ has all zero eigenvalues since otherwise the set of non-zero eigenvalues wouldn't be the same. Thus $\lambda_{max}(AA^T) = 0$. Therefore, in either case $\lambda_{max}(A^TA) = \lambda_{max}(AA^T)$. This proves that $||A||_2 = ||A^T||_2$. 
\end{proof}

\section*{Property 4}
\begin{align*}
||QAZ|| = ||A||
\end{align*}
where $Q \in \mathbb{R}^{m \times m}$ and $Z \in \mathbb{R}^{n \times n}$ are orthogonal matrices. The matrix norm here is either the Frobenius norm or the operator norm induced by $||\cdot||_2$. 

\begin{proof} Frobenius.
\\Let's recall the definition of the Frobenius Norm
\begin{align*}
||A||_F = \left( \sum_{j = 1}^n \sum_{i = 1}^m |a_{ij}|^2 \right)^{1/2} = \left( \sum_{j = 1}^n ||a_j||_2^2 \right)^{1/2} = \left( \sum_{i = 1}^m ||a_j||_2^2 \right)^{1/2}
\end{align*}
where $(a_j)$ is the $j$-th column of $A$. The second expression is what I will start using with the LHS of the desired equality:
\begin{align*}
||QAZ||_F^2 = \sum_{j = 1}^n ||r_j||_2^2
\end{align*}
where $(r_j)$ is the $j$-th column of $QAZ$. Let $(q_j) \subset \mathbb{R}^m$ be the $j$-th column of $Q$ and let $(y_{ij})$ be the $i,j$-th element of $AZ$, then we have that 
\begin{align*}
r_j = \sum_{k = 1}^m q_k y_{kj}
\end{align*}
Thus, we get
\begin{align*}
||QAZ||_F^2 &= \sum_{j = 1}^n \left| \left| \sum_{k = 1}^m q_k y_{kj} \right| \right|_2^2\\
&= \sum_{j = 1}^n \left\langle \sum_{k = 1}^m q_k y_{kj}, \sum_{k = 1}^m q_k y_{kj} \right \rangle\\
&= \sum_{j = 1}^n \sum_{k = 1}^m |y_{kj}|^2 ||q_k||_2^2 &\text{since the colums of $Q$ are orthogonal}\\
&= \sum_{j = 1}^n \sum_{k = 1}^m |y_{kj}|^2 &\text{since the columns of $Q$ are normal}\\
&= ||AZ||_F^2
\end{align*}
Similarly, let $(a_{ij})$ be the elements of $A$ and let $(z_i)$ be the rows of $Z$. Then, I will use the third expression for the Frobenius norm as expressed as the beginning to say that 
\begin{align*}
||AZ||_F^2 = \sum_{i = 1}^m ||y_i||_2^2
\end{align*}
where $(y_i)$ is the $i$-th row of $AZ$. Note we can express $y_i$ as 
\begin{align*}
y_i = \sum_{k = 1}^n z_k a_{ik}
\end{align*}
Therefore, we get
\begin{align*}
||AZ||_F^2 = \sum_{i = 1}^m \left| \left| \sum_{k = 1}^n z_k a_{ik} \right| \right|_2^2\\
&= \sum_{i = 1}^m \left \langle \sum_{k = 1}^n z_k a_{ik}, \sum_{k = 1}^n z_k a_{ik} \right \rangle\\
&= \sum_{i = 1}^m \sum_{k = 1}^n |a_{ik}|^2 ||z_k||^k &\text{since the rows of $Z$ are orthogonal}\\
&= \sum_{i = 1}^m \sum_{k = 1}^n |a_{ik}|^2 &\text{since the rows of $Z$ are normal}\\
&= ||A||_F^2
\end{align*}
\end{proof}

\begin{proof} Operator Norm.
\\Notice that 
\begin{align*}
||QAZ||_2 &= \max_{x \neq 0} \frac{||QAZx||_2}{||x||_2}\\
&= \max_{x \neq 0} \frac{||Q(AZx)||_2}{||x||_2}\\
&= \max_{x \neq 0} \frac{||AZx||_2}{||x||_2} &\text{since $Q$ is orthogonal, i.e. satisfies $||Qv|| = ||v||$}\\
&= ||AZ||_2
\end{align*}
Next, use property 3 to conclude that $||AZ||_2 = ||(AZ)^T||_2 = ||Z^T A^T||_2$. Note that $Z^T$ must also be an orthogonal matrix, so we can use an equivalent proof as above to conclude that 
\begin{align*}
||Z^T A^T||_2 = ||A^T||_2
\end{align*}
Thus, once again using property 3, we get the sequence of equalities
\begin{align*}
||QAZ||_2 = ||AZ||_2 = ||Z^T A^T||_2 = ||A^T||_2 = ||A||_2
\end{align*}
\end{proof}

\section*{Property 5}
\begin{align*}
||A||_2 = \max_{||x||_2 = 1} |x^T A x| \quad \text{if $A$ is symmetric}
\end{align*}

\begin{proof}$ $
\\Let $x$ be a unit vector. Since $A$ is symmetric, it has an orthonormal eigenbasis $\{v_i\}_{i = 1}^n$. Furthermore, each eigenvalue of $A$ must be non-negative. Thus, expressing $x$ in this eigenbasis gives:
\begin{align*}
x &= \sum_{i = 1}^n \alpha_i v_i\\
\implies x^T A x &= \sum_{i = 1}^n \alpha_i v_i^T \sum_{i = 1}^n \alpha_i A v_i\\
&= \sum_{i = 1}^n \alpha_i v_i^T \sum_{i = 1}^n \alpha_i \lambda_i v_i\\
&= \sum_{i = 1}^n \alpha_i^2 \lambda_i &\text{since the basis is orthonormal}
\end{align*}
Notice that this sum here is always positive and is maximized when $\alpha_i = 1$ for $i$ the index of the maximal eigenvalue and $\alpha_i = 0$ for all other indices. Thus, we get that 
\begin{align*}
\max_{||x||_2 = 1} |x^T A x| = \lambda_{max}(A)
\end{align*}
On the other hand, by property 2, we have
\begin{align*}
||A||_2 &= \sqrt{\lambda_{max}(A^T A)}\\
&= \sqrt{\lambda_{max}(A^2)}
\end{align*}
Furthermore, let $\lambda_0$ be the maximal eigenvalue of $A$ with eigenvector $v_0$. Then, we have that 
\begin{align*}
A^2 v_0 = A(Av_0) = A(\lambda_0 v_0) = \lambda_0 A v_0 = \lambda_0^2 v_0
\end{align*}
so that $\lambda_0^2$ is an eigenvalue of $A^2$ with the same eigenvector. Furthermore, $\lambda_0^2$ must be the maximal eigenvalue of $A^2$ since we can find that if $\lambda_i$ is an eigenvalue for $A$, then $\lambda_i^2$ is an eigenvalue for $A^2$ by the argument above. Thus, since each $\lambda_i$ is non-negative, the maximum of $\{\lambda_i^2\}$ corresponds to the square of the maximum of $\{\lambda_i\}$. Thus, we get
\begin{align*}
\sqrt{\lambda_{max}(A^2)} = \sqrt{\lambda_{max}(A)^2} = |\lambda_{max}(A)| = \lambda_{max}(A)
\end{align*}
Thus, we get
\begin{align*}
||A||_2 = \lambda_{max}(A) = \max_{||x||_2 = 1} |x^T A x|
\end{align*}
which finishes the proof
\end{proof}

\section*{Property 6}
\begin{align*}
\frac{1}{\sqrt{n}}||A||_2 \leq ||A||_1 \leq \sqrt{n}||A||_2
\end{align*}

\begin{proof}$ $
\\Recall the similar inequality for vector norms:
\begin{align}\label{INEQ1}
||x||_2 \leq ||x||_1 \leq \sqrt{n} ||x||_2
\end{align}
Thus, we have that
\begin{align*}
||A||_1 = \max_{x \neq 0} \frac{||Ax||_1}{||x||_1} &\leq \max_{x \neq 0} \frac{\sqrt{n}||Ax||_2}{||x||_1} &\text{by \eqref{INEQ1}}\\
&\leq \max_{x \neq 0} \frac{\sqrt{n}||Ax||_2}{||x||_2} &\text{by \eqref{INEQ1}}\\
&= \sqrt{n}||A||_2\\
||A||_1 = \max_{x \neq 0} \frac{||Ax||_1}{||x||_1} &\geq \max_{x \neq 0} \frac{||Ax||_1}{\sqrt{n}||x||_2} &\text{by \eqref{INEQ1}}\\
&\geq \max_{x \neq 0} \frac{||Ax||_2}{\sqrt{n}||x||_2} &\text{by \eqref{INEQ1}}\\
&= \frac{1}{\sqrt{n}} ||A||_2
\end{align*}
\end{proof}

\section*{Property 7}
\begin{align*}
\frac{1}{\sqrt{n}}||A||_2 \leq ||A||_\infty \leq \sqrt{n}||A||_2
\end{align*}

\begin{proof}$ $
\\Recall the similar property we have for vector norms:
\begin{align}\label{INEQ2}
||x||_\infty \leq ||x||_2 \leq \sqrt{n} ||x||_\infty
\end{align}
Thus, we have that
\begin{align*}
||A||_\infty = \max_{x \neq 0} \frac{||Ax||_\infty}{||x||_\infty} &\leq \max_{x \neq 0} \frac{||Ax||_2}{||x||_\infty} &\text{by \eqref{INEQ2}}\\
&\leq \max_{x \neq 0} \frac{||Ax||_2}{(1/\sqrt{n})||x||_2} &\text{by \eqref{INEQ2}}\\
&= \sqrt{n}||A||_2\\
||A||_\infty = \max_{x \neq 0} \frac{||Ax||_\infty}{||x||_\infty} &\geq \max_{x \neq 0} \frac{(1/\sqrt{n})||Ax||_2}{||x||_\infty} &\text{by \eqref{INEQ2}}\\
&\geq \max_{x \neq 0} \frac{||Ax||_2}{\sqrt{n}||x||_2} &\text{by \eqref{INEQ2}}\\
&= \frac{1}{\sqrt{n}} ||A||_2
\end{align*}
\end{proof}



\end{document}